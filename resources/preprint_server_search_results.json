[
  {
    "paper_id": "2412.18609v1",
    "title": "Video-Panda: Parameter-efficient Alignment for Encoder-free\n  Video-Language Models",
    "abstract": "We present an efficient encoder-free approach for video-language\nunderstanding that achieves competitive performance while significantly\nreducing computational overhead. Current video-language models typically rely\non heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B\nparameters), creating a substantial computational burden when processing\nmulti-frame videos. Our method introduces a novel Spatio-Temporal Alignment\nBlock (STAB) that directly processes video inputs without requiring pre-trained\nencoders while using only 45M parameters for visual processing - at least a\n6.5$\\times$ reduction compared to traditional approaches. The STAB architecture\ncombines Local Spatio-Temporal Encoding for fine-grained feature extraction,\nefficient spatial downsampling through learned attention and separate\nmechanisms for modeling frame-level and video-level relationships. Our model\nachieves comparable or superior performance to encoder-based approaches for\nopen-ended video question answering on standard benchmarks. The fine-grained\nvideo question-answering evaluation demonstrates our model's effectiveness,\noutperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key\naspects like correctness and temporal understanding. Extensive ablation studies\nvalidate our architectural choices and demonstrate the effectiveness of our\nspatio-temporal modeling approach while achieving 3-4$\\times$ faster processing\nspeeds than previous methods. Code is available at\n\\url{https://github.com/jh-yi/Video-Panda}.",
    "authors": [
      {
        "name": "Jinhui Yi"
      },
      {
        "name": "Syed Talal Wasim"
      },
      {
        "name": "Yanan Luo"
      },
      {
        "name": "Muzammal Naseer"
      },
      {
        "name": "Juergen Gall"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18609v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18608v1",
    "title": "PartGen: Part-level 3D Generation and Reconstruction with Multi-View\n  Diffusion Models",
    "abstract": "Text- or image-to-3D generators and 3D scanners can now produce 3D assets\nwith high-quality shapes and textures. These assets typically consist of a\nsingle, fused representation, like an implicit neural field, a Gaussian\nmixture, or a mesh, without any useful structure. However, most applications\nand creative workflows require assets to be made of several meaningful parts\nthat can be manipulated independently. To address this gap, we introduce\nPartGen, a novel approach that generates 3D objects composed of meaningful\nparts starting from text, an image, or an unstructured 3D object. First, given\nmultiple views of a 3D object, generated or rendered, a multi-view diffusion\nmodel extracts a set of plausible and view-consistent part segmentations,\ndividing the object into parts. Then, a second multi-view diffusion model takes\neach part separately, fills in the occlusions, and uses those completed views\nfor 3D reconstruction by feeding them to a 3D reconstruction network. This\ncompletion process considers the context of the entire object to ensure that\nthe parts integrate cohesively. The generative completion model can make up for\nthe information missing due to occlusions; in extreme cases, it can hallucinate\nentirely invisible parts based on the input 3D asset. We evaluate our method on\ngenerated and real 3D assets and show that it outperforms segmentation and\npart-extraction baselines by a large margin. We also showcase downstream\napplications such as 3D part editing.",
    "authors": [
      {
        "name": "Minghao Chen"
      },
      {
        "name": "Roman Shapovalov"
      },
      {
        "name": "Iro Laina"
      },
      {
        "name": "Tom Monnier"
      },
      {
        "name": "Jianyuan Wang"
      },
      {
        "name": "David Novotny"
      },
      {
        "name": "Andrea Vedaldi"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18608v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18607v1",
    "title": "DrivingGPT: Unifying Driving World Modeling and Planning with\n  Multi-modal Autoregressive Transformers",
    "abstract": "World model-based searching and planning are widely recognized as a promising\npath toward human-level physical intelligence. However, current driving world\nmodels primarily rely on video diffusion models, which specialize in visual\ngeneration but lack the flexibility to incorporate other modalities like\naction. In contrast, autoregressive transformers have demonstrated exceptional\ncapability in modeling multimodal data. Our work aims to unify both driving\nmodel simulation and trajectory planning into a single sequence modeling\nproblem. We introduce a multimodal driving language based on interleaved image\nand action tokens, and develop DrivingGPT to learn joint world modeling and\nplanning through standard next-token prediction. Our DrivingGPT demonstrates\nstrong performance in both action-conditioned video generation and end-to-end\nplanning, outperforming strong baselines on large-scale nuPlan and NAVSIM\nbenchmarks.",
    "authors": [
      {
        "name": "Yuntao Chen"
      },
      {
        "name": "Yuqi Wang"
      },
      {
        "name": "Zhaoxiang Zhang"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18607v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18606v1",
    "title": "Lattice T-duality from non-invertible symmetries in quantum spin chains",
    "abstract": "Dualities of quantum field theories are challenging to realize in lattice\nmodels of qubits. In this work, we explore one of the simplest dualities,\nT-duality of the compact boson CFT, and its realization in quantum spin chains.\nIn the special case of the XX model, we uncover an exact lattice T-duality,\nwhich is associated with a non-invertible symmetry that exchanges two lattice\nU(1) symmetries. The latter symmetries flow to the momentum and winding U(1)\nsymmetries with a mixed anomaly in the CFT. However, the charge operators of\nthe two U(1) symmetries do not commute on the lattice and instead generate the\nOnsager algebra. We discuss how some of the anomalies in the CFT are\nnonetheless still exactly realized on the lattice and how the lattice U(1)\nsymmetries enforce gaplessness. We further explore lattice deformations\npreserving both U(1) symmetries and find a rich gapless phase diagram with\nspecial $\\mathrm{Spin}(2k)_1$ WZW model points and whose phase transitions all\nhave dynamical exponent ${z>1}$.",
    "authors": [
      {
        "name": "Salvatore D. Pace"
      },
      {
        "name": "Arkya Chatterjee"
      },
      {
        "name": "Shu-Heng Shao"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18606v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18605v1",
    "title": "Orient Anything: Learning Robust Object Orientation Estimation from\n  Rendering 3D Models",
    "abstract": "Orientation is a key attribute of objects, crucial for understanding their\nspatial pose and arrangement in images. However, practical solutions for\naccurate orientation estimation from a single image remain underexplored. In\nthis work, we introduce Orient Anything, the first expert and foundational\nmodel designed to estimate object orientation in a single- and free-view image.\nDue to the scarcity of labeled data, we propose extracting knowledge from the\n3D world. By developing a pipeline to annotate the front face of 3D objects and\nrender images from random views, we collect 2M images with precise orientation\nannotations. To fully leverage the dataset, we design a robust training\nobjective that models the 3D orientation as probability distributions of three\nangles and predicts the object orientation by fitting these distributions.\nBesides, we employ several strategies to improve synthetic-to-real transfer.\nOur model achieves state-of-the-art orientation estimation accuracy in both\nrendered and real images and exhibits impressive zero-shot ability in various\nscenarios. More importantly, our model enhances many applications, such as\ncomprehension and generation of complex spatial concepts and 3D object pose\nadjustment.",
    "authors": [
      {
        "name": "Zehan Wang"
      },
      {
        "name": "Ziang Zhang"
      },
      {
        "name": "Tianyu Pang"
      },
      {
        "name": "Chao Du"
      },
      {
        "name": "Hengshuang Zhao"
      },
      {
        "name": "Zhou Zhao"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18605v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18604v1",
    "title": "Explaining in Diffusion: Explaining a Classifier Through Hierarchical\n  Semantics with Text-to-Image Diffusion Models",
    "abstract": "Classifiers are important components in many computer vision tasks, serving\nas the foundational backbone of a wide variety of models employed across\ndiverse applications. However, understanding the decision-making process of\nclassifiers remains a significant challenge. We propose DiffEx, a novel method\nthat leverages the capabilities of text-to-image diffusion models to explain\nclassifier decisions. Unlike traditional GAN-based explainability models, which\nare limited to simple, single-concept analyses and typically require training a\nnew model for each classifier, our approach can explain classifiers that focus\non single concepts (such as faces or animals) as well as those that handle\ncomplex scenes involving multiple concepts. DiffEx employs vision-language\nmodels to create a hierarchical list of semantics, allowing users to identify\nnot only the overarching semantic influences on classifiers (e.g., the 'beard'\nsemantic in a facial classifier) but also their sub-types, such as 'goatee' or\n'Balbo' beard. Our experiments demonstrate that DiffEx is able to cover a\nsignificantly broader spectrum of semantics compared to its GAN counterparts,\nproviding a hierarchical tool that delivers a more detailed and fine-grained\nunderstanding of classifier decisions.",
    "authors": [
      {
        "name": "Tahira Kazimi"
      },
      {
        "name": "Ritika Allada"
      },
      {
        "name": "Pinar Yanardag"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18604v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18603v1",
    "title": "Long-Form Speech Generation with Spoken Language Models",
    "abstract": "We consider the generative modeling of speech over multiple minutes, a\nrequirement for long-form multimedia generation and audio-native voice\nassistants. However, current spoken language models struggle to generate\nplausible speech past tens of seconds, from high temporal resolution of speech\ntokens causing loss of coherence, to architectural issues with long-sequence\ntraining or extrapolation, to memory costs at inference time. With these\nconsiderations we propose SpeechSSM, the first speech language model to learn\nfrom and sample long-form spoken audio (e.g., 16 minutes of read or\nextemporaneous speech) in a single decoding session without text intermediates,\nbased on recent advances in linear-time sequence modeling. Furthermore, to\naddress growing challenges in spoken language evaluation, especially in this\nnew long-form setting, we propose: new embedding-based and LLM-judged metrics;\nquality measurements over length and time; and a new benchmark for long-form\nspeech processing and generation, LibriSpeech-Long. Speech samples and the\ndataset are released at\nhttps://google.github.io/tacotron/publications/speechssm/",
    "authors": [
      {
        "name": "Se Jin Park"
      },
      {
        "name": "Julian Salazar"
      },
      {
        "name": "Aren Jansen"
      },
      {
        "name": "Keisuke Kinoshita"
      },
      {
        "name": "Yong Man Ro"
      },
      {
        "name": "RJ Skerry-Ryan"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18603v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18601v1",
    "title": "Decentralized Intelligence in GameFi: Embodied AI Agents and the\n  Convergence of DeFi and Virtual Ecosystems",
    "abstract": "In the rapidly evolving landscape of GameFi, a fusion of gaming and\ndecentralized finance (DeFi), there exists a critical need to enhance player\nengagement and economic interaction within gaming ecosystems. Our GameFi\necosystem aims to fundamentally transform this landscape by integrating\nadvanced embodied AI agents into GameFi platforms. These AI agents, developed\nusing cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,\nare capable of proactive, adaptive, and contextually rich interactions with\nplayers. By going beyond traditional scripted responses, these agents become\nintegral participants in the game's narrative and economic systems, directly\ninfluencing player strategies and in-game economies. We address the limitations\nof current GameFi platforms, which often lack immersive AI interactions and\nmechanisms for community engagement or creator monetization. Through the deep\nintegration of AI agents with blockchain technology, we establish a\nconsensus-driven, decentralized GameFi ecosystem. This ecosystem empowers\ncreators to monetize their contributions and fosters democratic collaboration\namong players and creators. Furthermore, by embedding DeFi mechanisms into the\ngaming experience, we enhance economic participation and provide new\nopportunities for financial interactions within the game. Our approach enhances\nplayer immersion and retention and advances the GameFi ecosystem by bridging\ntraditional gaming with Web3 technologies. By integrating sophisticated AI and\nDeFi elements, we contribute to the development of more engaging, economically\nrobust, and community-centric gaming environments. This project represents a\nsignificant advancement in the state-of-the-art in GameFi, offering insights\nand methodologies that can be applied throughout the gaming industry.",
    "authors": [
      {
        "name": "Fernando Jia"
      },
      {
        "name": "Jade Zheng"
      },
      {
        "name": "Florence Li"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18601v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18600v1",
    "title": "ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation",
    "abstract": "Human-scene interaction (HSI) generation is crucial for applications in\nembodied AI, virtual reality, and robotics. While existing methods can\nsynthesize realistic human motions in 3D scenes and generate plausible\nhuman-object interactions, they heavily rely on datasets containing paired 3D\nscene and motion capture data, which are expensive and time-consuming to\ncollect across diverse environments and interactions. We present ZeroHSI, a\nnovel approach that enables zero-shot 4D human-scene interaction synthesis by\nintegrating video generation and neural human rendering. Our key insight is to\nleverage the rich motion priors learned by state-of-the-art video generation\nmodels, which have been trained on vast amounts of natural human movements and\ninteractions, and use differentiable rendering to reconstruct human-scene\ninteractions. ZeroHSI can synthesize realistic human motions in both static\nscenes and environments with dynamic objects, without requiring any\nground-truth motion data. We evaluate ZeroHSI on a curated dataset of different\ntypes of various indoor and outdoor scenes with different interaction prompts,\ndemonstrating its ability to generate diverse and contextually appropriate\nhuman-scene interactions.",
    "authors": [
      {
        "name": "Hongjie Li"
      },
      {
        "name": "Hong-Xing Yu"
      },
      {
        "name": "Jiaman Li"
      },
      {
        "name": "Jiajun Wu"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18600v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18599v1",
    "title": "Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining\n  Rates with Ruin Theory",
    "abstract": "Theoretical guarantees for double spending probabilities for the Nakamoto\nconsensus under the $k$-deep confirmation rule have been extensively studied\nfor zero/bounded network delays and fixed mining rates. In this paper, we\nintroduce a ruin-theoretical model of double spending for Nakamoto consensus\nunder the $k$-deep confirmation rule when the honest mining rate is allowed to\nbe an arbitrary function of time including the block delivery periods, i.e.,\ntime periods during which mined blocks are being delivered to all other\nparticipants of the network. Time-varying mining rates are considered to\ncapture the intrinsic characteristics of the peer to peer network delays as\nwell as dynamic participation of miners such as the gap game and switching\nbetween different cryptocurrencies. Ruin theory is leveraged to obtain the\ndouble spend probabilities and numerical examples are presented to validate the\neffectiveness of the proposed analytical method.",
    "authors": [
      {
        "name": "Mustafa Doger"
      },
      {
        "name": "Sennur Ulukus"
      },
      {
        "name": "Nail Akar"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18599v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18597v1",
    "title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion\n  Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
    "abstract": "Sora-like video generation models have achieved remarkable progress with a\nMulti-Modal Diffusion Transformer MM-DiT architecture. However, the current\nvideo generation models predominantly focus on single-prompt, struggling to\ngenerate coherent scenes with multiple sequential prompts that better reflect\nreal-world dynamic scenarios. While some pioneering works have explored\nmulti-prompt video generation, they face significant challenges including\nstrict training data requirements, weak prompt following, and unnatural\ntransitions. To address these problems, we propose DiTCtrl, a training-free\nmulti-prompt video generation method under MM-DiT architectures for the first\ntime. Our key idea is to take the multi-prompt video generation task as\ntemporal video editing with smooth transitions. To achieve this goal, we first\nanalyze MM-DiT's attention mechanism, finding that the 3D full attention\nbehaves similarly to that of the cross/self-attention blocks in the UNet-like\ndiffusion models, enabling mask-guided precise semantic control across\ndifferent prompts with attention sharing for multi-prompt video generation.\nBased on our careful design, the video generated by DiTCtrl achieves smooth\ntransitions and consistent object motion given multiple sequential prompts\nwithout additional training. Besides, we also present MPVBench, a new benchmark\nspecially designed for multi-prompt video generation to evaluate the\nperformance of multi-prompt generation. Extensive experiments demonstrate that\nour method achieves state-of-the-art performance without additional training.",
    "authors": [
      {
        "name": "Minghong Cai"
      },
      {
        "name": "Xiaodong Cun"
      },
      {
        "name": "Xiaoyu Li"
      },
      {
        "name": "Wenze Liu"
      },
      {
        "name": "Zhaoyang Zhang"
      },
      {
        "name": "Yong Zhang"
      },
      {
        "name": "Ying Shan"
      },
      {
        "name": "Xiangyu Yue"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18597v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18596v1",
    "title": "LatentCRF: Continuous CRF for Efficient Latent Diffusion",
    "abstract": "Latent Diffusion Models (LDMs) produce high-quality, photo-realistic images,\nhowever, the latency incurred by multiple costly inference iterations can\nrestrict their applicability. We introduce LatentCRF, a continuous Conditional\nRandom Field (CRF) model, implemented as a neural network layer, that models\nthe spatial and semantic relationships among the latent vectors in the LDM. By\nreplacing some of the computationally-intensive LDM inference iterations with\nour lightweight LatentCRF, we achieve a superior balance between quality, speed\nand diversity. We increase inference efficiency by 33% with no loss in image\nquality or diversity compared to the full LDM. LatentCRF is an easy add-on,\nwhich does not require modifying the LDM.",
    "authors": [
      {
        "name": "Kanchana Ranasinghe"
      },
      {
        "name": "Sadeep Jayasumana"
      },
      {
        "name": "Andreas Veit"
      },
      {
        "name": "Ayan Chakrabarti"
      },
      {
        "name": "Daniel Glasner"
      },
      {
        "name": "Michael S Ryoo"
      },
      {
        "name": "Srikumar Ramalingam"
      },
      {
        "name": "Sanjiv Kumar"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18596v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18594v1",
    "title": "Structure Learning in Gaussian Graphical Models from Glauber Dynamics",
    "abstract": "Gaussian graphical model selection is an important paradigm with numerous\napplications, including biological network modeling, financial network\nmodeling, and social network analysis. Traditional approaches assume access to\nindependent and identically distributed (i.i.d) samples, which is often\nimpractical in real-world scenarios. In this paper, we address Gaussian\ngraphical model selection under observations from a more realistic dependent\nstochastic process known as Glauber dynamics. Glauber dynamics, also called the\nGibbs sampler, is a Markov chain that sequentially updates the variables of the\nunderlying model based on the statistics of the remaining model. Such models,\naside from frequently being employed to generate samples from complex\nmultivariate distributions, naturally arise in various settings, such as\nopinion consensus in social networks and clearing/stock-price dynamics in\nfinancial networks.\n  In contrast to the extensive body of existing work, we present the first\nalgorithm for Gaussian graphical model selection when data are sampled\naccording to the Glauber dynamics. We provide theoretical guarantees on the\ncomputational and statistical complexity of the proposed algorithm's structure\nlearning performance. Additionally, we provide information-theoretic lower\nbounds on the statistical complexity and show that our algorithm is nearly\nminimax optimal for a broad class of problems.",
    "authors": [
      {
        "name": "Vignesh Tirukkonda"
      },
      {
        "name": "Anirudh Rayas"
      },
      {
        "name": "Gautam Dasarathy"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18594v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18593v1",
    "title": "Modeling the Centaur: Human-Machine Synergy in Sequential Decision\n  Making",
    "abstract": "The field of collective intelligence studies how teams can achieve better\nresults than any of the team members alone. The special case of human-machine\nteams carries unique challenges in this regard. For example, human teams often\nachieve synergy by communicating to discover their relative advantages, which\nis not an option if the team partner is an unexplainable deep neural network.\nBetween 2005-2008 a set of \"freestyle\" chess tournaments were held, in which\nhuman-machine teams known as \"centaurs\", outperformed the best humans and best\nmachines alone. Centaur players reported that they identified relative\nadvantages between themselves and their chess program, even though the program\nwas superhuman. Inspired by this and leveraging recent open-source models, we\nstudy human-machine like teams in chess. A human behavioral clone (\"Maia\") and\na pure self-play RL-trained chess engine (\"Leela\") were composed into a team\nusing a Mixture of Experts (MoE) architecture. By directing our research\nquestion at the selection mechanism of the MoE, we could isolate the issue of\nextracting relative advantages without knowledge sharing. We show that in\nprinciple, there is high potential for synergy between human and machine in a\ncomplex sequential decision environment such as chess. Furthermore, we show\nthat an expert can identify only a small part of these relative advantages, and\nthat the contribution of its subject matter expertise in doing so saturates\nquickly. This is probably due to the \"curse of knowledge\" phenomenon. We also\ntrain a network to recognize relative advantages using reinforcement learning,\nwithout chess expertise, and it outdoes the expert. Our experiments are\nrepeated in asymmetric teams, in which identifying relative advantages is more\nchallenging. Our findings contribute to the study of collective intelligence\nand human-centric AI.",
    "authors": [
      {
        "name": "David Shoresh"
      },
      {
        "name": "Yonatan Loewenstein"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18593v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18591v1",
    "title": "ClassifyViStA:WCE Classification with Visual understanding through\n  Segmentation and Attention",
    "abstract": "Gastrointestinal (GI) bleeding is a serious medical condition that presents\nsignificant diagnostic challenges, particularly in settings with limited access\nto healthcare resources. Wireless Capsule Endoscopy (WCE) has emerged as a\npowerful diagnostic tool for visualizing the GI tract, but it requires\ntime-consuming manual analysis by experienced gastroenterologists, which is\nprone to human error and inefficient given the increasing number of patients.To\naddress this challenge, we propose ClassifyViStA, an AI-based framework\ndesigned for the automated detection and classification of bleeding and\nnon-bleeding frames from WCE videos. The model consists of a standard\nclassification path, augmented by two specialized branches: an implicit\nattention branch and a segmentation branch.The attention branch focuses on the\nbleeding regions, while the segmentation branch generates accurate segmentation\nmasks, which are used for classification and interpretability. The model is\nbuilt upon an ensemble of ResNet18 and VGG16 architectures to enhance\nclassification performance. For the bleeding region detection, we implement a\nSoft Non-Maximum Suppression (Soft NMS) approach with YOLOv8, which improves\nthe handling of overlapping bounding boxes, resulting in more accurate and\nnuanced detections.The system's interpretability is enhanced by using the\nsegmentation masks to explain the classification results, offering insights\ninto the decision-making process similar to the way a gastroenterologist\nidentifies bleeding regions. Our approach not only automates the detection of\nGI bleeding but also provides an interpretable solution that can ease the\nburden on healthcare professionals and improve diagnostic efficiency. Our code\nis available at ClassifyViStA.",
    "authors": [
      {
        "name": "S. Balasubramanian"
      },
      {
        "name": "Ammu Abhishek"
      },
      {
        "name": "Yedu Krishna"
      },
      {
        "name": "Darshan Gera"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18591v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18588v1",
    "title": "A Paragraph is All It Takes: Rich Robot Behaviors from Interacting,\n  Trusted LLMs",
    "abstract": "Large Language Models (LLMs) are compact representations of all public\nknowledge of our physical environment and animal and human behaviors. The\napplication of LLMs to robotics may offer a path to highly capable robots that\nperform well across most human tasks with limited or even zero tuning. Aside\nfrom increasingly sophisticated reasoning and task planning, networks of\n(suitably designed) LLMs offer ease of upgrading capabilities and allow humans\nto directly observe the robot's thinking. Here we explore the advantages,\nlimitations, and particularities of using LLMs to control physical robots. The\nbasic system consists of four LLMs communicating via a human language data bus\nimplemented via web sockets and ROS2 message passing. Surprisingly, rich robot\nbehaviors and good performance across different tasks could be achieved despite\nthe robot's data fusion cycle running at only 1Hz and the central data bus\nrunning at the extremely limited rates of the human brain, of around 40 bits/s.\nThe use of natural language for inter-LLM communication allowed the robot's\nreasoning and decision making to be directly observed by humans and made it\ntrivial to bias the system's behavior with sets of rules written in plain\nEnglish. These rules were immutably written into Ethereum, a global, public,\nand censorship resistant Turing-complete computer. We suggest that by using\nnatural language as the data bus among interacting AIs, and immutable public\nledgers to store behavior constraints, it is possible to build robots that\ncombine unexpectedly rich performance, upgradability, and durable alignment\nwith humans.",
    "authors": [
      {
        "name": " OpenMind"
      },
      {
        "name": "Shaohong Zhong"
      },
      {
        "name": "Adam Zhou"
      },
      {
        "name": "Boyuan Chen"
      },
      {
        "name": "Homin Luo"
      },
      {
        "name": "Jan Liphardt"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18588v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18584v1",
    "title": "Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors:\n  Diverse-Resolution Training Outperforms Interpolation",
    "abstract": "Deep learning-based 3D imaging, in particular magnetic resonance imaging\n(MRI), is challenging because of limited availability of 3D training data.\nTherefore, 2D diffusion models trained on 2D slices are starting to be\nleveraged for 3D MRI reconstruction. However, as we show in this paper,\nexisting methods pertain to a fixed voxel size, and performance degrades when\nthe voxel size is varied, as it is often the case in clinical practice. In this\npaper, we propose and study several approaches for resolution-robust 3D MRI\nreconstruction with 2D diffusion priors. As a result of this investigation, we\nobtain a simple resolution-robust variational 3D reconstruction approach based\non diffusion-guided regularization of randomly sampled 2D slices. This method\nprovides competitive reconstruction quality compared to posterior sampling\nbaselines. Towards resolving the sensitivity to resolution-shifts, we\ninvestigate state-of-the-art model-based approaches including Gaussian\nsplatting, neural representations, and infinite-dimensional diffusion models,\nas well as a simple data-centric approach of training the diffusion model on\nseveral resolutions. Our experiments demonstrate that the model-based\napproaches fail to close the performance gap in 3D MRI. In contrast, the\ndata-centric approach of training the diffusion model on various resolutions\neffectively provides a resolution-robust method without compromising accuracy.",
    "authors": [
      {
        "name": "Anselm Krainovic"
      },
      {
        "name": "Stefan Ruschke"
      },
      {
        "name": "Reinhard Heckel"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18584v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18582v1",
    "title": "Exploring Embedding Priors in Prompt-Tuning for Improved\n  Interpretability and Control",
    "abstract": "Prompt-Tuning is an efficient method for adapting pre-trained language models\nto new tasks with minimal computational overhead by modifying prompt\nembeddings. In this work, we investigate how crucial the phenomenon of\nembedding collapse, frequently observed in Prompt-Tuning, is for the final\nperformance of the model. To address this question, we designed embedding\npriors and compared them with posteriors of the converged Soft and Deep\nPrompt-Tuning methods. Our findings suggest that priors strongly affect the\nposition of the tuned embeddings, and models can effectively work with\nembeddings from different parts of activation spaces, including completely new\nregions. As the final Prompt-Tuning capabilities are limited, we hypothesize\nthat controllable Prompt-Tuning posteriors may serve as a good starting point\nfor tasks such as chain-of-thought (COT) distillation. Our experiments also\nshow that generated trajectories are not localized in the activation space of\nthe models. However, there are distinct clusters of activations for distant\ntasks (e.g., NLP and arithmetic), while activations between NLP tasks (e.g.,\nQuestion-Answering and MLM) lie in the same cluster. These observations raise\nquestions about the importance of a single activation cluster for the\ngeneralization abilities of large language models.",
    "authors": [
      {
        "name": "Sergey Sedov"
      },
      {
        "name": "Sumanth Bharadwaj Hachalli Karanam"
      },
      {
        "name": "Venu Gopal Kadamba"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18582v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18580v1",
    "title": "A mathematical framework for modelling CLMM dynamics in continuous time",
    "abstract": "This paper develops a rigorous mathematical framework for analyzing\nConcentrated Liquidity Market Makers (CLMMs) in Decentralized Finance (DeFi)\nwithin a continuous-time setting. We model the evolution of liquidity profiles\nas measure-valued processes and characterize their dynamics under continuous\ntrading. Our analysis encompasses two critical aspects of CLMMs: the mechanics\nof concentrated liquidity provision and the strategic behavior of arbitrageurs.\nWe examine three distinct arbitrage models -- myopic, finite-horizon, and\ninfinite-horizon with discounted and ergodic controls -- and derive closed-form\nsolutions for optimal arbitrage strategies under each scenario. Importantly, we\ndemonstrate that the presence of trading fees fundamentally constrains the\nadmissible price processes, as the inclusion of fees precludes the existence of\ndiffusion terms in the price process to avoid infinite fee generation. This\nfinding has significant implications for CLMM design and market efficiency.",
    "authors": [
      {
        "name": "Shen-Ning Tung"
      },
      {
        "name": "Tai-Ho Wang"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18580v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18579v1",
    "title": "ReducedLUT: Table Decomposition with \"Don't Care\" Conditions",
    "abstract": "Lookup tables (LUTs) are frequently used to efficiently store arrays of\nprecomputed values for complex mathematical computations. When used in the\ncontext of neural networks, these functions exhibit a lack of recognizable\npatterns which presents an unusual challenge for conventional logic synthesis\ntechniques. Several approaches are known to break down a single large lookup\ntable into multiple smaller ones that can be recombined. Traditional methods,\nsuch as plain tabulation, piecewise linear approximation, and multipartite\ntable methods, often yield inefficient hardware solutions when applied to\nLUT-based NNs.\n  This paper introduces ReducedLUT, a novel method to reduce the footprint of\nthe LUTs by injecting don't cares into the compression process. This additional\nfreedom introduces more self-similarities which can be exploited using known\ndecomposition techniques. We then demonstrate a particular application to\nmachine learning; by replacing unobserved patterns within the training data of\nneural network models with don't cares, we enable greater compression with\nminimal model accuracy degradation. In practice, we achieve up to $1.63\\times$\nreduction in Physical LUT utilization, with a test accuracy drop of no more\nthan $0.01$ accuracy points.",
    "authors": [
      {
        "name": "Oliver Cassidy"
      },
      {
        "name": "Marta Andronic"
      },
      {
        "name": "Samuel Coward"
      },
      {
        "name": "George A. Constantinides"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18579v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18577v1",
    "title": "New Insights on Gamma-Ray Burst Radiation Mechanisms from\n  Multiwavelength Observations",
    "abstract": "The study of high-energy gamma-ray emission from gamma-ray bursts (GRBs)\ninvolves complex synchrotron radiation and synchrotron self-Compton scattering\n(SSC) mechanisms with multiple parameters exhibiting a wide distribution.\nRecent advancements in GRB research, particularly the observation of very high\nenergy (VHE, $\\rm >100~GeV$) radiation, have ushered in a new era of\nmultiwavelength exploration, offering fresh perspectives and limitations for\nunderstanding GRB radiation mechanisms. This study aimed to leverage VHE\nobservations to refine constraints on synchrotron + SSC radiation from\nelectrons accelerated by forward shocks. By analyzing two external environments\n- the uniform interstellar medium and stratified stellar wind medium, we\nconducted spectral and variability fitting for five specific bursts\n(GRB~180720B, GRB~190114C, GRB~190829A, GRB~201216C, and GRB~221009A) to\nidentify the optimal parameters characterizing these events. A comparative\nanalysis of model parameter distributions with and without VHE radiation\nobservations reveals that the magnetic energy equipartition factor $\\epsilon_B$\nis more concentrated with VHE emissions. This suggests that VHE emissions may\noffer greater constraints on this microphysical parameter. Additionally, we\nfound that the energy budget between VHE and keV-MeV $\\gamma$-ray emissions\nunder the SSC radiation exhibits an almost linear relationship, which may serve\nas a tool to differentiate radiation mechanisms. We anticipate future\nstatistical analyses of additional VHE bursts to validate our findings.",
    "authors": [
      {
        "name": "Yu-Hua Yao"
      },
      {
        "name": "Fang-Sheng Min"
      },
      {
        "name": "Shi Chen"
      },
      {
        "name": "Yi-Qing Guo"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18577v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18576v1",
    "title": "Machine Learning Approaches to the Shafarevich-Tate Group of Elliptic\n  Curves",
    "abstract": "We train machine learning models to predict the order of the Shafarevich-Tate\ngroup of an elliptic curve over $\\mathbb{Q}$. Building on earlier work of He,\nLee, and Oliver, we show that a feed-forward neural network classifier trained\non subsets of the invariants arising in the Birch--Swinnerton-Dyer conjectural\nformula yields higher accuracies ($> 0.9$) than any model previously studied.\nIn addition, we develop a regression model that may be used to predict orders\nof this group not seen during training and apply this to the elliptic curve of\nrank 29 recently discovered by Elkies and Klagsbrun. Finally we conduct some\nexploratory data analyses and visualizations on our dataset. We use the\nelliptic curve dataset from the L-functions and modular forms database (LMFDB).",
    "authors": [
      {
        "name": "Angelica Babei"
      },
      {
        "name": "Barinder S. Banwait"
      },
      {
        "name": "AJ Fong"
      },
      {
        "name": "Xiaoyu Huang"
      },
      {
        "name": "Deependra Singh"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18576v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18573v1",
    "title": "How Well Do LLMs Generate Code for Different Application Domains?\n  Benchmark and Evaluation",
    "abstract": "Recently, an increasing number of AI-driven programming assistants powered by\ncode LLMs have been integrated into various real-world software development\nenvironments, significantly boosting developer productivity. However, existing\ncode generation benchmarks primarily focus on general-purpose scenarios,\nleaving the code generation performance of LLMs for specific application\ndomains largely unknown. In this paper, we introduce a new benchmark,\nMultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming\ntasks, covering 12 popular software development domains and 15 programming\nlanguages. Specifically, we perform in-depth research to identify these 12\napplication domains. Given that each domain may involve multiple technical\nframeworks, and that different frameworks present distinct challenges in the\ncoding process, we categorize the commonly used frameworks and platforms within\neach domain. We then sample programming problems from GitHub repositories\nrelated to these subdomains. To ensure the quality of the tasks and mitigate\ndata leakage issues, we invite annotators to rewrite the docstrings for each\ntask in MultiCodeBench. Additionally, we build a static analysis-based\ndependency parsing tool to extract the dependencies in the ground truth for\neach task, enabling deeper performance analysis. Through extensive experiments\non MultiCodeBench with eleven representative mainstream LLMs, we reveal the\ncode generation performance of the LLMs across different application domains,\nproviding practical insights for developers in downstream fields when selecting\nLLMs. Furthermore, we analyze the reasons behind the models' failures in\ncompleting software application development tasks, offering guidance for model\ndevelopers to enhance domain-specific code generation capabilities.",
    "authors": [
      {
        "name": "Dewu Zheng"
      },
      {
        "name": "Yanlin Wang"
      },
      {
        "name": "Ensheng Shi"
      },
      {
        "name": "Hongyu Zhang"
      },
      {
        "name": "Zibin Zheng"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18573v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18571v1",
    "title": "Scalable Quantum-Inspired Optimization through Dynamic Qubit Compression",
    "abstract": "Hard combinatorial optimization problems, often mapped to Ising models,\npromise potential solutions with quantum advantage but are constrained by\nlimited qubit counts in near-term devices. We present an innovative\nquantum-inspired framework that dynamically compresses large Ising models to\nfit available quantum hardware of different sizes. Thus, we aim to bridge the\ngap between large-scale optimization and current hardware capabilities. Our\nmethod leverages a physics-inspired GNN architecture to capture complex\ninteractions in Ising models and accurately predict alignments among\nneighboring spins (aka qubits) at ground states. By progressively merging such\naligned spins, we can reduce the model size while preserving the underlying\noptimization structure. It also provides a natural trade-off between the\nsolution quality and size reduction, meeting different hardware constraints of\nquantum computing devices. Extensive numerical studies on Ising instances of\ndiverse topologies show that our method can reduce instance size at multiple\nlevels with virtually no losses in solution quality on the latest D-wave\nquantum annealers.",
    "authors": [
      {
        "name": "Co Tran"
      },
      {
        "name": "Quoc-Bao Tran"
      },
      {
        "name": "Hy Truong Son"
      },
      {
        "name": "Thang N Dinh"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18571v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18568v1",
    "title": "HNCI: High-Dimensional Network Causal Inference",
    "abstract": "The problem of evaluating the effectiveness of a treatment or policy commonly\nappears in causal inference applications under network interference. In this\npaper, we suggest the new method of high-dimensional network causal inference\n(HNCI) that provides both valid confidence interval on the average direct\ntreatment effect on the treated (ADET) and valid confidence set for the\nneighborhood size for interference effect. We exploit the model setting in\nBelloni et al. (2022) and allow certain type of heterogeneity in node\ninterference neighborhood sizes. We propose a linear regression formulation of\npotential outcomes, where the regression coefficients correspond to the\nunderlying true interference function values of nodes and exhibit a latent\nhomogeneous structure. Such a formulation allows us to leverage existing\nliterature from linear regression and homogeneity pursuit to conduct valid\nstatistical inferences with theoretical guarantees. The resulting confidence\nintervals for the ADET are formally justified through asymptotic normalities\nwith estimable variances. We further provide the confidence set for the\nneighborhood size with theoretical guarantees exploiting the repro samples\napproach. The practical utilities of the newly suggested methods are\ndemonstrated through simulation and real data examples.",
    "authors": [
      {
        "name": "Wenqin Du"
      },
      {
        "name": "Rundong Ding"
      },
      {
        "name": "Yingying Fan"
      },
      {
        "name": "Jinchi Lv"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18568v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18566v1",
    "title": "Zero-resource Speech Translation and Recognition with LLMs",
    "abstract": "Despite recent advancements in speech processing, zero-resource speech\ntranslation (ST) and automatic speech recognition (ASR) remain challenging\nproblems. In this work, we propose to leverage a multilingual Large Language\nModel (LLM) to perform ST and ASR in languages for which the model has never\nseen paired audio-text data. We achieve this by using a pre-trained\nmultilingual speech encoder, a multilingual LLM, and a lightweight adaptation\nmodule that maps the audio representations to the token embedding space of the\nLLM. We perform several experiments both in ST and ASR to understand how to\nbest train the model and what data has the most impact on performance in\npreviously unseen languages. In ST, our best model is capable to achieve BLEU\nscores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we\nachieve WERs of up to 28.2\\%. We finally show that the performance of our\nsystem is bounded by the ability of the LLM to output text in the desired\nlanguage.",
    "authors": [
      {
        "name": "Karel Mundnich"
      },
      {
        "name": "Xing Niu"
      },
      {
        "name": "Prashant Mathur"
      },
      {
        "name": "Srikanth Ronanki"
      },
      {
        "name": "Brady Houston"
      },
      {
        "name": "Veera Raghavendra Elluru"
      },
      {
        "name": "Nilaksh Das"
      },
      {
        "name": "Zejiang Hou"
      },
      {
        "name": "Goeric Huybrechts"
      },
      {
        "name": "Anshu Bhatia"
      },
      {
        "name": "Daniel Garcia-Romero"
      },
      {
        "name": "Kyu J. Han"
      },
      {
        "name": "Katrin Kirchhoff"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18566v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18565v1",
    "title": "3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement",
    "abstract": "Despite advances in neural rendering, due to the scarcity of high-quality 3D\ndatasets and the inherent limitations of multi-view diffusion models, view\nsynthesis and 3D model generation are restricted to low resolutions with\nsuboptimal multi-view consistency. In this study, we present a novel 3D\nenhancement pipeline, dubbed 3DEnhancer, which employs a multi-view latent\ndiffusion model to enhance coarse 3D inputs while preserving multi-view\nconsistency. Our method includes a pose-aware encoder and a diffusion-based\ndenoiser to refine low-quality multi-view images, along with data augmentation\nand a multi-view attention module with epipolar aggregation to maintain\nconsistent, high-quality 3D outputs across views. Unlike existing video-based\napproaches, our model supports seamless multi-view enhancement with improved\ncoherence across diverse viewing angles. Extensive evaluations show that\n3DEnhancer significantly outperforms existing methods, boosting both multi-view\nenhancement and per-instance 3D optimization tasks.",
    "authors": [
      {
        "name": "Yihang Luo"
      },
      {
        "name": "Shangchen Zhou"
      },
      {
        "name": "Yushi Lan"
      },
      {
        "name": "Xingang Pan"
      },
      {
        "name": "Chen Change Loy"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18565v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18564v1",
    "title": "Efficient Aircraft Design Optimization Using Multi-Fidelity Models and\n  Multi-fidelity Physics Informed Neural Networks",
    "abstract": "Aircraft design optimization traditionally relies on computationally\nexpensive simulation techniques such as Finite Element Method (FEM) and Finite\nVolume Method (FVM), which, while accurate, can significantly slow down the\ndesign iteration process. The challenge lies in reducing the computational\ncomplexity while maintaining high accuracy for quick evaluations of multiple\ndesign alternatives. This research explores advanced methods, including\nsurrogate models, reduced-order models (ROM), and multi-fidelity machine\nlearning techniques, to achieve more efficient aircraft design evaluations.\nSpecifically, the study investigates the application of Multi-fidelity\nPhysics-Informed Neural Networks (MPINN) and autoencoders for manifold\nalignment, alongside the potential of Generative Adversarial Networks (GANs)\nfor refining design geometries. Through a proof-of-concept task, the research\ndemonstrates the ability to predict high-fidelity results from low-fidelity\nsimulations, offering a path toward faster and more cost effective aircraft\ndesign iterations.",
    "authors": [
      {
        "name": "Apurba Sarker"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18564v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18563v1",
    "title": "Dynamic Optimization of Portfolio Allocation Using Deep Reinforcement\n  Learning",
    "abstract": "Artificial intelligence is fundamentally transforming financial investment\ndecision-making paradigms, with deep reinforcement learning (DRL) demonstrating\nsignificant application potential in domains such as robo-advisory services.\nGiven that traditional portfolio optimization methods face significant\nchallenges in effectively managing dynamic asset weight adjustments, this paper\napproaches the problem from the perspective of practical trading processes and\ndevelops a dynamic optimization model using deep reinforcement learning to\nachieve more effective asset allocation. The study's innovations are twofold:\nFirst, we propose a Sharpe ratio reward function specifically designed for\nActor-Critic deep reinforcement learning algorithms, which optimizes portfolio\nperformance by maximizing the average Sharpe ratio through random sampling and\nreinforcement learning algorithms during the training process; Second, we\ndesign deep neural networks that are specifically structured to meet asset\noptimization objectives. The study empirically evaluates the model using\nrandomly selected constituent stocks from the CSI300 index and conducts\ncomparative analyses against traditional approaches, including mean-variance\noptimization and risk parity strategies. Backtesting results demonstrate the\ndynamic optimization model's effectiveness in portfolio asset allocation,\nyielding enhanced risk reduction, superior risk-return metrics, and optimal\nperformance across comprehensive evaluation criteria.",
    "authors": [
      {
        "name": "Gang Huang"
      },
      {
        "name": "Xiaohua Zhou"
      },
      {
        "name": "Qingyang Song"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18563v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18561v1",
    "title": "Fano physics behind the N-resonance in graphene",
    "abstract": "Bound states and scattering resonances in the unoccupied continuum of a\ntwo-dimensional crystal predicted in [Phys$.$Rev$.$ B 87, 041405(R) (2013)] are\nconsidered within an exactly solvable model. A close connection of the observed\nresonances with those arising in the Fano theory is revealed. The resonance\noccurs when the lateral scattering couples the layer-perpendicular incident\nelectron wave to a strictly bound state. The coupling strength determines the\nlocation of the pole in the scattering amplitude in the complex energy plane,\nwhich is analytically shown to lead to a characteristic Fano-lineshape of the\nenergy dependence of the electron transmissivity through the crystal. The\nimplications for the timing of the resonance scattering are discussed. The\nanalytical results are illustrated by ab initio calculations for a graphene\nmonolayer.",
    "authors": [
      {
        "name": "R. O. Kuzian"
      },
      {
        "name": "D. V. Efremov"
      },
      {
        "name": "E. E. Krasovskii"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18561v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18560v1",
    "title": "Adapting Priority Riemann Solver for GSOM on road networks",
    "abstract": "In this paper, we present an extension of the Generic Second Order Models\n(GSOM) for traffic flow on road networks. We define a Riemann solver at the\njunction based on a priority rule and provide an iterative algorithm to\nconstruct solutions at junctions with n incoming and m outgoing roads. The\nlogic underlying our solver is as follows: the flow is maximized while\nrespecting the priority rule, which can be adjusted if the supply of an\noutgoing road exceeds the demand of a higher-priority incoming road.\nApproximate solutions for Cauchy problems are constructed using wave-front\ntracking. We establish bounds on the total variation of waves interacting with\nthe junction and present explicit calculations for junctions with two incoming\nand two outgoing roads. A key novelty of this work is the detailed analysis of\nreturning waves - waves generated at the junction that return to the junction\nafter interacting along the roads - which, in contrast to first-order models\nsuch as LWR, can increase flux variation.",
    "authors": [
      {
        "name": "Caterina Balzotti"
      },
      {
        "name": "Roberta Bianchini"
      },
      {
        "name": "Maya Briani"
      },
      {
        "name": "Benedetto Piccoli"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18560v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18557v1",
    "title": "FedVCK: Non-IID Robust and Communication-Efficient Federated Learning\n  via Valuable Condensed Knowledge for Medical Image Analysis",
    "abstract": "Federated learning has become a promising solution for collaboration among\nmedical institutions. However, data owned by each institution would be highly\nheterogeneous and the distribution is always non-independent and identical\ndistribution (non-IID), resulting in client drift and unsatisfactory\nperformance. Despite existing federated learning methods attempting to solve\nthe non-IID problems, they still show marginal advantages but rely on frequent\ncommunication which would incur high costs and privacy concerns. In this paper,\nwe propose a novel federated learning method: \\textbf{Fed}erated learning via\n\\textbf{V}aluable \\textbf{C}ondensed \\textbf{K}nowledge (FedVCK). We enhance\nthe quality of condensed knowledge and select the most necessary knowledge\nguided by models, to tackle the non-IID problem within limited communication\nbudgets effectively. Specifically, on the client side, we condense the\nknowledge of each client into a small dataset and further enhance the\ncondensation procedure with latent distribution constraints, facilitating the\neffective capture of high-quality knowledge. During each round, we specifically\ntarget and condense knowledge that has not been assimilated by the current\nmodel, thereby preventing unnecessary repetition of homogeneous knowledge and\nminimizing the frequency of communications required. On the server side, we\npropose relational supervised contrastive learning to provide more supervision\nsignals to aid the global model updating. Comprehensive experiments across\nvarious medical tasks show that FedVCK can outperform state-of-the-art methods,\ndemonstrating that it's non-IID robust and communication-efficient.",
    "authors": [
      {
        "name": "Guochen Yan"
      },
      {
        "name": "Luyuan Xie"
      },
      {
        "name": "Xinyi Gao"
      },
      {
        "name": "Wentao Zhang"
      },
      {
        "name": "Qingni Shen"
      },
      {
        "name": "Yuejian Fang"
      },
      {
        "name": "Zhonghai Wu"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18557v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2412.18555v1",
    "title": "Analysis of non-overlapping models with a weighted infinite delay",
    "abstract": "The framework of this article is cell motility modeling. Approximating cells\nas rigid spheres we take into account for both non-penetration and adhesions\nforces. Adhesions are modeled as a memory-like microscopic elastic forces. This\nleads to a delayed and constrained vector valued system of equations. We prove\nthat the solution of these equations converges when {\\epsilon}, the linkages\nturnover parameter, tends to zero to the a constrained model with friction. We\ndiscretize the problem and penalize the constraints to get an uncon?strained\nminimization problem. The well-posedness of the constrained problem is obtained\nby letting the penalty parameter to tend to zero. Energy estimates `a la De\nGiorgi are derived accounting for delay. Thanks to these estimates and the\nconvexity of the constraints, we obtain compactness uniformly with respect to\nthe discretisation step and {\\epsilon}, this is the mathematically involved\npart of the article. Considering that the characteristic bonds lifetime goes to\nzero, we recover a friction model comparable to [Venel et al, ESAIM, 2011] but\nunder more realistic assumptions on the external load, this part being also one\nof the challenging aspects of the work",
    "authors": [
      {
        "name": "Thierno Mamadou Balde"
      },
      {
        "name": "Vuk Milisic"
      }
    ],
    "url": "http://arxiv.org/pdf/2412.18555v1",
    "venue": "arXiv",
    "year": 2024,
    "publicationDate": "2024-12-24"
  },
  {
    "paper_id": "2024.10.24.619766",
    "title": "CpGPT: a Foundation Model for DNA Methylation",
    "abstract": "",
    "authors": [
      {
        "name": "Lucas Paulo de Lima Camillo"
      },
      {
        "name": "Raghav Sehgal"
      },
      {
        "name": "Jenel Armstrong"
      },
      {
        "name": "Albert T. Higgins-Chen"
      },
      {
        "name": "Steve Horvath"
      },
      {
        "name": "Bo Wang"
      }
    ],
    "url": "https://www.biorxiv.org/content/10.1101/2024.10.24.619766",
    "venue": "Biorxiv",
    "year": 2024,
    "publicationDate": "2024-10-24"
  },
  {
    "paper_id": "2024.12.10.627665",
    "title": "ProCyon: A multimodal foundation model for protein phenotypes",
    "abstract": "",
    "authors": [
      {
        "name": "Owen Queen"
      },
      {
        "name": "Yepeng Huang"
      },
      {
        "name": "Robert Calef"
      },
      {
        "name": "Valentina Giunchiglia"
      },
      {
        "name": "Tianlong Chen"
      },
      {
        "name": "George Dasoulas"
      },
      {
        "name": "LeAnn Tai"
      },
      {
        "name": "Yasha Ektefaie"
      },
      {
        "name": "Ayush Noori"
      },
      {
        "name": "Joseph Brown"
      },
      {
        "name": "Tom Cobley"
      },
      {
        "name": "Karin Hrovatin"
      },
      {
        "name": "Tom Hartvigsen"
      },
      {
        "name": "Fabian J. Theis"
      },
      {
        "name": "Bradley Pentelute"
      },
      {
        "name": "Vikram Khurana"
      },
      {
        "name": "Manolis Kellis"
      },
      {
        "name": "Marinka Zitnik"
      }
    ],
    "url": "https://www.biorxiv.org/content/10.1101/2024.12.10.627665",
    "venue": "Biorxiv",
    "year": 2024,
    "publicationDate": "2024-12-10"
  },
  {
    "paper_id": "2024.05.22.594943",
    "title": "A generative foundation model for antibody sequence understanding",
    "abstract": "",
    "authors": [
      {
        "name": "Justin Barton"
      },
      {
        "name": "Aretas Gaspariunas"
      },
      {
        "name": "David A. Yadin"
      },
      {
        "name": "Jorge Dias"
      },
      {
        "name": "Francesca L. Nice"
      },
      {
        "name": "Danielle H. Minns"
      },
      {
        "name": "Olivia Snudden"
      },
      {
        "name": "Chelsea Povall"
      },
      {
        "name": "Sara Valle Tomas"
      },
      {
        "name": "Harry Dobson"
      },
      {
        "name": "James H. R. Farmery"
      },
      {
        "name": "Jinwoo Leem"
      },
      {
        "name": "Jacob D. Galson"
      }
    ],
    "url": "https://www.biorxiv.org/content/10.1101/2024.05.22.594943",
    "venue": "Biorxiv",
    "year": 2024,
    "publicationDate": "2024-05-22"
  },
  {
    "paper_id": "2024.07.30.605655",
    "title": "Video Foundation Models for Animal Behavior Analysis",
    "abstract": "",
    "authors": [
      {
        "name": "Jennifer J. Sun"
      },
      {
        "name": "Hao Zhou"
      },
      {
        "name": "Long Zhao"
      },
      {
        "name": "Liangzhe Yuan"
      },
      {
        "name": "Bryan Seybold"
      },
      {
        "name": "David Hendon"
      },
      {
        "name": "Florian Schroff"
      },
      {
        "name": "David A. Ross"
      },
      {
        "name": "Hartwig Adam"
      },
      {
        "name": "Bo Hu"
      },
      {
        "name": "Ting Liu"
      }
    ],
    "url": "https://www.biorxiv.org/content/10.1101/2024.07.30.605655",
    "venue": "Biorxiv",
    "year": 2024,
    "publicationDate": "2024-07-30"
  },
  {
    "paper_id": "2024.04.15.589472",
    "title": "Nicheformer: a foundation model for single-cell and spatial omics",
    "abstract": "",
    "authors": [
      {
        "name": "Anna C. Schaar"
      },
      {
        "name": "Alejandro Tejada-Lapuerta"
      },
      {
        "name": "Giovanni Palla"
      },
      {
        "name": "Robert Gutgesell"
      },
      {
        "name": "Lennard Halle"
      },
      {
        "name": "Mariia Minaeva"
      },
      {
        "name": "Larsen Vornholz"
      },
      {
        "name": "Leander Dony"
      },
      {
        "name": "Francesca Drummer"
      },
      {
        "name": "Mojtaba Bahrami"
      },
      {
        "name": "Fabian J. Theis"
      }
    ],
    "url": "https://www.biorxiv.org/content/10.1101/2024.04.15.589472",
    "venue": "Biorxiv",
    "year": 2024,
    "publicationDate": "2024-04-15"
  },
  {
    "paper_id": "2024.10.06.616870",
    "title": "ProteinAligner: A Multi-modal Pretraining Framework for Protein Foundation Models",
    "abstract": "",
    "authors": [
      {
        "name": "Li Zhang"
      },
      {
        "name": "Han Guo"
      },
      {
        "name": "Leah Schaffer"
      },
      {
        "name": "Young Su Ko"
      },
      {
        "name": "Digvijay Singh"
      },
      {
        "name": "Hamid Rahmani"
      },
      {
        "name": "Danielle Grotjahn"
      },
      {
        "name": "Elizabeth Villa"
      },
      {
        "name": "Michael Gilson"
      },
      {
        "name": "Wei Wang"
      },
      {
        "name": "Trey Ideker"
      },
      {
        "name": "Eric Xing"
      },
      {
        "name": "Pengtao Xie"
      }
    ],
    "url": "https://www.biorxiv.org/content/10.1101/2024.10.06.616870",
    "venue": "Biorxiv",
    "year": 2024,
    "publicationDate": "2024-10-06"
  },
  {
    "paper_id": "2023.11.28.568918",
    "title": "Universal Cell Embeddings: A Foundation Model for Cell Biology",
    "abstract": "",
    "authors": [
      {
        "name": "Yanay Rosen"
      },
      {
        "name": "Yusuf Roohani"
      },
      {
        "name": "Ayush Agrawal"
      },
      {
        "name": "Leon Samotorčan"
      },
      {
        "name": "Tabula Sapiens Consortium"
      },
      {
        "name": "Stephen R. Quake"
      },
      {
        "name": "Jure Leskovec"
      }
    ],
    "url": "https://www.biorxiv.org/content/10.1101/2023.11.28.568918",
    "venue": "Biorxiv",
    "year": 2023,
    "publicationDate": "2023-11-28"
  },
  {
    "paper_id": "2024.12.12.628025",
    "title": "FeatureForest: the power of foundation models, the usability of random forests",
    "abstract": "",
    "authors": [
      {
        "name": "Mehdi Seifi"
      },
      {
        "name": "Damian Dalle Nogare"
      },
      {
        "name": "Juan Battagliotti"
      },
      {
        "name": "Vera Galinova"
      },
      {
        "name": "Ananya Kedige Rao"
      },
      {
        "name": "AI4Life Horizon Europe Programme Consortium"
      },
      {
        "name": "Johan Decelle"
      },
      {
        "name": "Florian Jug"
      },
      {
        "name": "Joran Deschamps"
      }
    ],
    "url": "https://www.biorxiv.org/content/10.1101/2024.12.12.628025",
    "venue": "Biorxiv",
    "year": 2024,
    "publicationDate": "2024-12-12"
  },
  {
    "paper_id": "2024.12.13.628448",
    "title": "Evaluating the role of pre-training dataset size and diversity on single-cell foundation model performance",
    "abstract": "",
    "authors": [
      {
        "name": "Alan DenAdel"
      },
      {
        "name": "Madeline Hughes"
      },
      {
        "name": "Akshaya Thoutam"
      },
      {
        "name": "Anay Gupta"
      },
      {
        "name": "Andrew W. Navia"
      },
      {
        "name": "Nicolo Fusi"
      },
      {
        "name": "Srivatsan Raghavan"
      },
      {
        "name": "Peter S. Winter"
      },
      {
        "name": "Ava P. Amini"
      },
      {
        "name": "Lorin Crawford"
      }
    ],
    "url": "https://www.biorxiv.org/content/10.1101/2024.12.13.628448",
    "venue": "Biorxiv",
    "year": 2024,
    "publicationDate": "2024-12-13"
  },
  {
    "paper_id": "2024.09.09.612009",
    "title": "Novae: a graph-based foundation model for spatial transcriptomics data",
    "abstract": "",
    "authors": [
      {
        "name": "Quentin Blampey"
      },
      {
        "name": "Hakim Benkirane"
      },
      {
        "name": "Nadège Bercovici"
      },
      {
        "name": "Fabrice André"
      },
      {
        "name": "Paul-Henry Cournède"
      }
    ],
    "url": "https://www.biorxiv.org/content/10.1101/2024.09.09.612009",
    "venue": "Biorxiv",
    "year": 2024,
    "publicationDate": "2024-09-09"
  },
  {
    "paper_id": "2024.10.24.619766",
    "title": "CpGPT: a Foundation Model for DNA Methylation",
    "abstract": "",
    "authors": [
      {
        "name": "Lucas Paulo de Lima Camillo"
      },
      {
        "name": "Raghav Sehgal"
      },
      {
        "name": "Jenel Armstrong"
      },
      {
        "name": "Albert T. Higgins-Chen"
      },
      {
        "name": "Steve Horvath"
      },
      {
        "name": "Bo Wang"
      }
    ],
    "url": "https://www.medrxiv.org/content/10.1101/2024.10.24.619766",
    "venue": "Medrxiv",
    "year": 2024,
    "publicationDate": "2024-10-24"
  },
  {
    "paper_id": "2024.12.10.627665",
    "title": "ProCyon: A multimodal foundation model for protein phenotypes",
    "abstract": "",
    "authors": [
      {
        "name": "Owen Queen"
      },
      {
        "name": "Yepeng Huang"
      },
      {
        "name": "Robert Calef"
      },
      {
        "name": "Valentina Giunchiglia"
      },
      {
        "name": "Tianlong Chen"
      },
      {
        "name": "George Dasoulas"
      },
      {
        "name": "LeAnn Tai"
      },
      {
        "name": "Yasha Ektefaie"
      },
      {
        "name": "Ayush Noori"
      },
      {
        "name": "Joseph Brown"
      },
      {
        "name": "Tom Cobley"
      },
      {
        "name": "Karin Hrovatin"
      },
      {
        "name": "Tom Hartvigsen"
      },
      {
        "name": "Fabian J. Theis"
      },
      {
        "name": "Bradley Pentelute"
      },
      {
        "name": "Vikram Khurana"
      },
      {
        "name": "Manolis Kellis"
      },
      {
        "name": "Marinka Zitnik"
      }
    ],
    "url": "https://www.medrxiv.org/content/10.1101/2024.12.10.627665",
    "venue": "Medrxiv",
    "year": 2024,
    "publicationDate": "2024-12-10"
  },
  {
    "paper_id": "2024.05.22.594943",
    "title": "A generative foundation model for antibody sequence understanding",
    "abstract": "",
    "authors": [
      {
        "name": "Justin Barton"
      },
      {
        "name": "Aretas Gaspariunas"
      },
      {
        "name": "David A. Yadin"
      },
      {
        "name": "Jorge Dias"
      },
      {
        "name": "Francesca L. Nice"
      },
      {
        "name": "Danielle H. Minns"
      },
      {
        "name": "Olivia Snudden"
      },
      {
        "name": "Chelsea Povall"
      },
      {
        "name": "Sara Valle Tomas"
      },
      {
        "name": "Harry Dobson"
      },
      {
        "name": "James H. R. Farmery"
      },
      {
        "name": "Jinwoo Leem"
      },
      {
        "name": "Jacob D. Galson"
      }
    ],
    "url": "https://www.medrxiv.org/content/10.1101/2024.05.22.594943",
    "venue": "Medrxiv",
    "year": 2024,
    "publicationDate": "2024-05-22"
  },
  {
    "paper_id": "2024.07.30.605655",
    "title": "Video Foundation Models for Animal Behavior Analysis",
    "abstract": "",
    "authors": [
      {
        "name": "Jennifer J. Sun"
      },
      {
        "name": "Hao Zhou"
      },
      {
        "name": "Long Zhao"
      },
      {
        "name": "Liangzhe Yuan"
      },
      {
        "name": "Bryan Seybold"
      },
      {
        "name": "David Hendon"
      },
      {
        "name": "Florian Schroff"
      },
      {
        "name": "David A. Ross"
      },
      {
        "name": "Hartwig Adam"
      },
      {
        "name": "Bo Hu"
      },
      {
        "name": "Ting Liu"
      }
    ],
    "url": "https://www.medrxiv.org/content/10.1101/2024.07.30.605655",
    "venue": "Medrxiv",
    "year": 2024,
    "publicationDate": "2024-07-30"
  },
  {
    "paper_id": "2024.04.15.589472",
    "title": "Nicheformer: a foundation model for single-cell and spatial omics",
    "abstract": "",
    "authors": [
      {
        "name": "Anna C. Schaar"
      },
      {
        "name": "Alejandro Tejada-Lapuerta"
      },
      {
        "name": "Giovanni Palla"
      },
      {
        "name": "Robert Gutgesell"
      },
      {
        "name": "Lennard Halle"
      },
      {
        "name": "Mariia Minaeva"
      },
      {
        "name": "Larsen Vornholz"
      },
      {
        "name": "Leander Dony"
      },
      {
        "name": "Francesca Drummer"
      },
      {
        "name": "Mojtaba Bahrami"
      },
      {
        "name": "Fabian J. Theis"
      }
    ],
    "url": "https://www.medrxiv.org/content/10.1101/2024.04.15.589472",
    "venue": "Medrxiv",
    "year": 2024,
    "publicationDate": "2024-04-15"
  },
  {
    "paper_id": "2024.10.06.616870",
    "title": "ProteinAligner: A Multi-modal Pretraining Framework for Protein Foundation Models",
    "abstract": "",
    "authors": [
      {
        "name": "Li Zhang"
      },
      {
        "name": "Han Guo"
      },
      {
        "name": "Leah Schaffer"
      },
      {
        "name": "Young Su Ko"
      },
      {
        "name": "Digvijay Singh"
      },
      {
        "name": "Hamid Rahmani"
      },
      {
        "name": "Danielle Grotjahn"
      },
      {
        "name": "Elizabeth Villa"
      },
      {
        "name": "Michael Gilson"
      },
      {
        "name": "Wei Wang"
      },
      {
        "name": "Trey Ideker"
      },
      {
        "name": "Eric Xing"
      },
      {
        "name": "Pengtao Xie"
      }
    ],
    "url": "https://www.medrxiv.org/content/10.1101/2024.10.06.616870",
    "venue": "Medrxiv",
    "year": 2024,
    "publicationDate": "2024-10-06"
  },
  {
    "paper_id": "2023.11.28.568918",
    "title": "Universal Cell Embeddings: A Foundation Model for Cell Biology",
    "abstract": "",
    "authors": [
      {
        "name": "Yanay Rosen"
      },
      {
        "name": "Yusuf Roohani"
      },
      {
        "name": "Ayush Agrawal"
      },
      {
        "name": "Leon Samotorčan"
      },
      {
        "name": "Tabula Sapiens Consortium"
      },
      {
        "name": "Stephen R. Quake"
      },
      {
        "name": "Jure Leskovec"
      }
    ],
    "url": "https://www.medrxiv.org/content/10.1101/2023.11.28.568918",
    "venue": "Medrxiv",
    "year": 2023,
    "publicationDate": "2023-11-28"
  },
  {
    "paper_id": "2024.12.12.628025",
    "title": "FeatureForest: the power of foundation models, the usability of random forests",
    "abstract": "",
    "authors": [
      {
        "name": "Mehdi Seifi"
      },
      {
        "name": "Damian Dalle Nogare"
      },
      {
        "name": "Juan Battagliotti"
      },
      {
        "name": "Vera Galinova"
      },
      {
        "name": "Ananya Kedige Rao"
      },
      {
        "name": "AI4Life Horizon Europe Programme Consortium"
      },
      {
        "name": "Johan Decelle"
      },
      {
        "name": "Florian Jug"
      },
      {
        "name": "Joran Deschamps"
      }
    ],
    "url": "https://www.medrxiv.org/content/10.1101/2024.12.12.628025",
    "venue": "Medrxiv",
    "year": 2024,
    "publicationDate": "2024-12-12"
  },
  {
    "paper_id": "2024.12.13.628448",
    "title": "Evaluating the role of pre-training dataset size and diversity on single-cell foundation model performance",
    "abstract": "",
    "authors": [
      {
        "name": "Alan DenAdel"
      },
      {
        "name": "Madeline Hughes"
      },
      {
        "name": "Akshaya Thoutam"
      },
      {
        "name": "Anay Gupta"
      },
      {
        "name": "Andrew W. Navia"
      },
      {
        "name": "Nicolo Fusi"
      },
      {
        "name": "Srivatsan Raghavan"
      },
      {
        "name": "Peter S. Winter"
      },
      {
        "name": "Ava P. Amini"
      },
      {
        "name": "Lorin Crawford"
      }
    ],
    "url": "https://www.medrxiv.org/content/10.1101/2024.12.13.628448",
    "venue": "Medrxiv",
    "year": 2024,
    "publicationDate": "2024-12-13"
  },
  {
    "paper_id": "2024.09.09.612009",
    "title": "Novae: a graph-based foundation model for spatial transcriptomics data",
    "abstract": "",
    "authors": [
      {
        "name": "Quentin Blampey"
      },
      {
        "name": "Hakim Benkirane"
      },
      {
        "name": "Nadège Bercovici"
      },
      {
        "name": "Fabrice André"
      },
      {
        "name": "Paul-Henry Cournède"
      }
    ],
    "url": "https://www.medrxiv.org/content/10.1101/2024.09.09.612009",
    "venue": "Medrxiv",
    "year": 2024,
    "publicationDate": "2024-09-09"
  }
]
